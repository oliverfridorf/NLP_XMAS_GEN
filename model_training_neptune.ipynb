{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 2 model training for XMAS generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import neptune.new as neptune\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model, pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "## setup device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model from pretrained GPT2 model. We use with LM(language model) head\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# set tokenizer padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# load pretrained model from huggingface\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "The data is prepared using a Dataset class and split up into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup datasets\n",
    "class lyricsdataset(Dataset):\n",
    "    def __init__(self, data_full, tokenizer, transform=None):\n",
    "        \n",
    "        self.data_split = data_full\n",
    "        self.token = tokenizer\n",
    "\n",
    "        # collect all lyrics into one string\n",
    "        self.string = ' '.join(self.data_split)\n",
    "        \n",
    "        # split string into list of lyrics\n",
    "        self.data = self.string.split('#CLAES#')\n",
    "        \n",
    "        # find max length of lyrics\n",
    "        self.length = np.zeros((len(self.data),1))\n",
    "        for i in range(len(self.data)):\n",
    "            self.length[i] = len(re.findall(r' ', self.data[i]))\n",
    "\n",
    "        self.max_length = int(np.max(self.length))  \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # last lyric is empty so we subtract 1\n",
    "        return len(self.data) - 1\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):            \n",
    "        \n",
    "        self.data_input = self.data[idx]\n",
    "        \n",
    "        # return tokenize indexed lyrics\n",
    "        return self.token(self.data_input, padding=\"max_length\", max_length=self.max_length,truncation = True,return_tensors=\"pt\" )\n",
    "\n",
    "# load data from file:\n",
    "with open(\"xmas_lyrics_dlpl.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "\n",
    "# create dataset with Dataset subclass\n",
    "dataset = lyricsdataset(raw_data,tokenizer)\n",
    "\n",
    "\n",
    "# implement dataset split using sklearns train test split function\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "# get dataloaders for training and testing\n",
    "train_data_loader = DataLoader(train_data)\n",
    "test_data_loader = DataLoader(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neptune setup\n",
    "Used for logging data about our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/XMAS/DLXMAS/e/DLXMAS-47\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"XMAS/DLXMAS\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjMzI3MjcxNy02ZmMzLTRmNDEtOWI5MS1mOGJkNTI2ZTgzMDkifQ==\",\n",
    ")  # Lasse Bjørnskov\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.001, \n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"batch_size\": 64\n",
    "}\n",
    "\n",
    "run[\"parameters\"] = params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Where the magic happens. The data is set into Dataloaders and the hyperparameters are set. Then the TRAINING begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 451.48 Eval loss: 74.59\n",
      "Epoch: 1 loss: 247.4 Eval loss: 66.86\n",
      "Epoch: 2 loss: 169.36 Eval loss: 68.37\n",
      "Epoch: 3 loss: 125.85 Eval loss: 68.85\n",
      "Epoch: 4 loss: 97.4 Eval loss: 70.02\n",
      "Epoch: 5 loss: 81.16 Eval loss: 73.18\n",
      "Epoch: 6 loss: 65.95 Eval loss: 78.47\n",
      "Epoch: 7 loss: 52.27 Eval loss: 75.13\n",
      "Epoch: 8 loss: 45.43 Eval loss: 80.02\n",
      "Epoch: 9 loss: 42.25 Eval loss: 83.62\n",
      "Epoch: 10 loss: 38.51 Eval loss: 87.37\n",
      "Epoch: 11 loss: 37.26 Eval loss: 85.99\n",
      "Epoch: 12 loss: 33.02 Eval loss: 87.1\n",
      "Epoch: 13 loss: 31.29 Eval loss: 87.0\n",
      "Epoch: 14 loss: 29.55 Eval loss: 86.6\n",
      "Epoch: 15 loss: 27.92 Eval loss: 86.2\n",
      "Epoch: 16 loss: 25.89 Eval loss: 98.37\n",
      "Epoch: 17 loss: 26.55 Eval loss: 92.0\n",
      "Epoch: 18 loss: 26.95 Eval loss: 88.74\n",
      "Epoch: 19 loss: 24.84 Eval loss: 94.17\n",
      "Epoch: 20 loss: 23.3 Eval loss: 91.38\n",
      "Epoch: 21 loss: 22.98 Eval loss: 95.93\n",
      "Epoch: 22 loss: 23.87 Eval loss: 92.91\n",
      "Epoch: 23 loss: 22.43 Eval loss: 100.49\n",
      "Epoch: 24 loss: 28.82 Eval loss: 104.29\n",
      "Epoch: 25 loss: 26.42 Eval loss: 91.22\n",
      "Epoch: 26 loss: 20.94 Eval loss: 100.9\n",
      "Epoch: 27 loss: 17.95 Eval loss: 99.98\n",
      "Epoch: 28 loss: 15.9 Eval loss: 102.83\n",
      "Epoch: 29 loss: 14.36 Eval loss: 105.17\n",
      "Epoch: 30 loss: 15.23 Eval loss: 105.08\n",
      "Epoch: 31 loss: 15.03 Eval loss: 105.65\n",
      "Epoch: 32 loss: 15.05 Eval loss: 103.66\n",
      "Epoch: 33 loss: 15.06 Eval loss: 102.71\n",
      "Epoch: 34 loss: 17.06 Eval loss: 102.81\n",
      "Epoch: 35 loss: 18.6 Eval loss: 97.14\n",
      "Epoch: 36 loss: 16.15 Eval loss: 104.22\n",
      "Epoch: 37 loss: 14.48 Eval loss: 106.4\n",
      "Epoch: 38 loss: 12.68 Eval loss: 104.62\n",
      "Epoch: 39 loss: 13.58 Eval loss: 104.95\n",
      "Epoch: 40 loss: 16.61 Eval loss: 110.3\n",
      "Epoch: 41 loss: 17.02 Eval loss: 112.5\n",
      "Epoch: 42 loss: 13.83 Eval loss: 103.28\n",
      "Epoch: 43 loss: 11.56 Eval loss: 107.83\n",
      "Epoch: 44 loss: 10.37 Eval loss: 107.99\n",
      "Epoch: 45 loss: 9.9 Eval loss: 108.31\n",
      "Epoch: 46 loss: 11.48 Eval loss: 110.93\n",
      "Epoch: 47 loss: 11.08 Eval loss: 106.05\n",
      "Epoch: 48 loss: 11.58 Eval loss: 103.96\n",
      "Epoch: 49 loss: 12.54 Eval loss: 112.62\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 5 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 5 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/XMAS/DLXMAS/e/DLXMAS-47\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "# hyperparameters\n",
    "nEpochs = 50\n",
    "batchSize = params[\"batch_size\"]\n",
    "learningRate = params[\"learning_rate\"]\n",
    "\n",
    "# For timing training\n",
    "start = time.time()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# optimizer to be used for training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learningRate)\n",
    "\n",
    "# loss for plotting\n",
    "epoch_loss = 0\n",
    "epoch_eval_loss = 0\n",
    "for iEpoch in range(nEpochs):\n",
    "    model.train()\n",
    "    for Xbatch in train_data_loader:\n",
    "\n",
    "        Xbatch = Xbatch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**Xbatch, labels=Xbatch[\"input_ids\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        epoch_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for Xbatch in test_data_loader:\n",
    "        Xbatch = Xbatch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(**Xbatch, labels=Xbatch[\"input_ids\"])\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        epoch_eval_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    print(f'Epoch: {iEpoch} loss: {np.round(epoch_loss, 2)} Eval loss: {np.round(epoch_eval_loss, 2)}')\n",
    "    \n",
    "    \n",
    "    # log loss to neptune\n",
    "    run[\"train/loss\"].log(epoch_loss)\n",
    "    run[\"Eval/loss\"].log(epoch_eval_loss)\n",
    "    \n",
    "    # rest epoch loss\n",
    "    epoch_loss = 0\n",
    "    epoch_eval_loss = 0\n",
    "    \n",
    "    if iEpoch % 5 == 0:\n",
    "        path = \"model\" + str(iEpoch) + \"epoch.pt\"\n",
    "        model_name = \"model\" + str(iEpoch) + \"epoch\"\n",
    "        torch.save(model, path)\n",
    "        run[model_name].upload(path)\n",
    "    \n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "#model.save_pretrained()\n",
    "model=model.cpu()\n",
    "torch.save(model, \"model_5epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song nr:1\n",
      "\n",
      "joyfull Mr. Kringle is soon gonna jingle\n",
      " The bells that'll tingle all your troubles away\n",
      " Everyone's waiting for the man with the bag\n",
      " Cause Christmas is coming again\n",
      " He's got a sleighful, it's not gonna stay full\n",
      " He's got stuff to drop at every stop on the way\n",
      " Everyone's waiting for the man with the bag\n",
      " Cause Christmas is coming again\n",
      " He'll be here with the answers to the prayers\n",
      " That you made through the year\n",
      " You'll get yours if you've done\n",
      " Everything you should, extra special good\n",
      " He'll make this December the one you'll remember\n",
      " The best and the merriest you ever did have\n",
      " Everybody's waiting for the man with the bag\n",
      " Christmas is here again\n",
      " He's got a sleighful and it's not gonna stay full\n",
      " He's got stuff to drop at every stop on the way\n",
      " Everybody's waiting for the man with the bag\n",
      " Christmas is here again\n",
      "\n",
      "song nr:2\n",
      "\n",
      "joyfull night, holy night\n",
      " All is calm, All is calm\n",
      " All is sleeping, All is bright\n",
      " 'Round yon virgin Mother and Child\n",
      " Holy infant so tender and mild\n",
      " Sleep in heavenly, heavenly, heavenly, la-la-la, la-la-la-la-la\n",
      " Holy, glorious, joyful, when Christ is born\n",
      " O tidings of comfort and joy\n",
      " Comfort and joy\n",
      " O tidings of comfort and joy\n",
      " In Bethlehem, in Israel, we are dream, by the fire\n",
      " To face unafraid, the plans that we've made\n",
      " Walking in a dream, by the fire\n",
      " To face unafraid, the plans that we made\n",
      " Walking in a dream, wonderland\n",
      " In the dark, we go, you high\n",
      " Let it snow is sleepin', snow is falling outside\n",
      " Come on, dear\n",
      " Gosh, Before Him lowly bend!\n",
      " Truly He taught us to love one another\n",
      " His law is love and\n"
     ]
    }
   ],
   "source": [
    "#model=model.cpu()\n",
    "pipe = pipeline(\"text-generation\", model = model, tokenizer=tokenizer)\n",
    "# test the model\n",
    "sent = 2\n",
    "gen_song = pipe(\"joyfull\", max_length=200, num_return_sequences=sent, temperature=1)\n",
    "for i in range(sent):\n",
    "    print(\"song nr:\" + str(i+1) + \"\\n\")\n",
    "    print(gen_song[i][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5ed7c0d65a1c7db7564ceef7683ac5f5d94f581ee5acd57e09a9a493aad0c6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
